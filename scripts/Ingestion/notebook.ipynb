{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/bitnami/spark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /root/.ivy2/cache\n",
      "The jars for the packages stored in: /root/.ivy2/jars\n",
      "org.mongodb.spark#mongo-spark-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-29908b01-3a2f-4fa4-9cd0-47c12d616dde;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.mongodb.spark#mongo-spark-connector_2.12;10.4.1 in central\n",
      "\tfound org.mongodb#mongodb-driver-sync;5.1.4 in central\n",
      "\t[5.1.4] org.mongodb#mongodb-driver-sync;[5.1.1,5.1.99)\n",
      "\tfound org.mongodb#bson;5.1.4 in central\n",
      "\tfound org.mongodb#mongodb-driver-core;5.1.4 in central\n",
      "\tfound org.mongodb#bson-record-codec;5.1.4 in central\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/spark/mongo-spark-connector_2.12/10.4.1/mongo-spark-connector_2.12-10.4.1.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb.spark#mongo-spark-connector_2.12;10.4.1!mongo-spark-connector_2.12.jar (150ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-sync/5.1.4/mongodb-driver-sync-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-sync;5.1.4!mongodb-driver-sync.jar (162ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson/5.1.4/bson-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson;5.1.4!bson.jar (262ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/mongodb-driver-core/5.1.4/mongodb-driver-core-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#mongodb-driver-core;5.1.4!mongodb-driver-core.jar (696ms)\n",
      "downloading https://repo1.maven.org/maven2/org/mongodb/bson-record-codec/5.1.4/bson-record-codec-5.1.4.jar ...\n",
      "\t[SUCCESSFUL ] org.mongodb#bson-record-codec;5.1.4!bson-record-codec.jar (48ms)\n",
      ":: resolution report :: resolve 3163ms :: artifacts dl 1331ms\n",
      "\t:: modules in use:\n",
      "\torg.mongodb#bson;5.1.4 from central in [default]\n",
      "\torg.mongodb#bson-record-codec;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-core;5.1.4 from central in [default]\n",
      "\torg.mongodb#mongodb-driver-sync;5.1.4 from central in [default]\n",
      "\torg.mongodb.spark#mongo-spark-connector_2.12;10.4.1 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   5   |   5   |   5   |   0   ||   5   |   5   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-29908b01-3a2f-4fa4-9cd0-47c12d616dde\n",
      "\tconfs: [default]\n",
      "\t5 artifacts copied, 0 already retrieved (2524kB/12ms)\n",
      "25/02/22 11:41:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-------------------+-------------------+--------+----------+--------+-----+------+--------------------+\n",
      "|                 _id|                body|          closed_at|         created_at|duration|  issue_id|language|stars| state|               title|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------+----------+--------+-----+------+--------------------+\n",
      "|67b24e4fd9191f330...|libpcap returned ...|2023-02-08 00:00:00|2023-02-06 00:00:00|       1|1573263352|    Rust|21527|closed|Errors occured wh...|\n",
      "|67b24e4fd9191f330...|thread 'thread_wr...|2023-02-25 00:00:00|2023-01-05 00:00:00|       8|1520037594|    Rust|21527|closed|thread 'thread_wr...|\n",
      "|67b24e4fd9191f330...|installing with n...|2023-05-24 00:00:00|2022-12-31 00:00:00|       5|1515053158|    Rust|21527|closed|unable to install...|\n",
      "|67b24e4fd9191f330...|\"Open full report...|2023-02-08 00:00:00|2022-12-22 00:00:00|       1|1507660835|    Rust|21527|closed|The \"Open full re...|\n",
      "|67b24e4fd9191f330...|wpcap can be a li...|2023-06-01 00:00:00|2022-12-21 00:00:00|       6|1506901817|    Rust|21527|closed|Include WPCAP on ...|\n",
      "+--------------------+--------------------+-------------------+-------------------+--------+----------+--------+-----+------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nombre total d'issues: 63126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+-----+\n",
      "|        language|count|\n",
      "+----------------+-----+\n",
      "|      JavaScript|28369|\n",
      "|               C|22401|\n",
      "|      TypeScript| 3806|\n",
      "|            Dart| 1785|\n",
      "|            Rust| 1749|\n",
      "|             MDX| 1577|\n",
      "|          Svelte| 1174|\n",
      "|           Swift|  573|\n",
      "|             Lua|  521|\n",
      "|            Java|  470|\n",
      "|Jupyter Notebook|  388|\n",
      "|          Python|  235|\n",
      "|                |   78|\n",
      "+----------------+-----+\n",
      "\n",
      "+----------------+------------------+\n",
      "|        language|      avg_duration|\n",
      "+----------------+------------------+\n",
      "|                | 4.602564102564102|\n",
      "|             MDX| 4.585922637920102|\n",
      "|            Dart| 4.572549019607843|\n",
      "|          Python|4.5574468085106385|\n",
      "|            Rust| 4.543739279588336|\n",
      "|          Svelte| 4.541737649063032|\n",
      "|      TypeScript|  4.51602732527588|\n",
      "|      JavaScript| 4.512108287214918|\n",
      "|Jupyter Notebook| 4.494845360824742|\n",
      "|               C|4.4901120485692605|\n",
      "|            Java| 4.446808510638298|\n",
      "|             Lua| 4.357005758157389|\n",
      "|           Swift| 4.273996509598604|\n",
      "+----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, desc\n",
    "import pymongo\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Création de la session Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MongoDB_Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.mongodb.spark:mongo-spark-connector_2.12:10.4.1\") \\\n",
    "    .config(\"spark.mongodb.read.connection.uri\", \"mongodb://mongodb:27017/github_issues\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Lecture des issues depuis MongoDB\n",
    "df_issues = spark.read.format(\"mongodb\") \\\n",
    "    .option(\"database\", \"github_issues\") \\\n",
    "    .option(\"collection\", \"closed_issues\") \\\n",
    "    .load()\n",
    "\n",
    "# Afficher un aperçu des données\n",
    "df_issues.show(5)\n",
    "\n",
    "# Compter le nombre total d'issues\n",
    "issue_count = df_issues.count()\n",
    "print(f\"Nombre total d'issues: {issue_count}\")\n",
    "\n",
    "# Nombre d'issues par langage\n",
    "df_language_count = df_issues.groupBy(\"language\").agg(count(\"issue_id\").alias(\"count\")).orderBy(desc(\"count\"))\n",
    "df_language_count.show()\n",
    "\n",
    "# Durée moyenne de fermeture des issues par langage\n",
    "df_duration_avg = df_issues.groupBy(\"language\").agg(avg(\"duration\").alias(\"avg_duration\")).orderBy(desc(\"avg_duration\"))\n",
    "df_duration_avg.show()\n",
    "\n",
    "# Arrêter la session Spark\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
