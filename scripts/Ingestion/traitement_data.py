from pyspark.sql import SparkSession

# Cr√©ation de la session Spark avec connexion au Master du cluster
spark = SparkSession.builder \
    .appName("MongoDB_Spark") \
    .master("spark://spark-master:7077") \
    .config("spark.jars.packages", "org.mongodb.spark:mongo-spark-connector_2.12:10.4.1") \
    .config("spark.mongodb.read.connection.uri", "mongodb://mongodb:27017/github_issues") \
    .config("spark.mongodb.write.connection.uri", "mongodb://mongodb:27017/github_issues") \
    .config("spark.executor.memory", "1G") \
    .config("spark.executor.cores", "1") \
    .config("spark.cores.max", "4") \
    .config("spark.driver.memory", "2G") \
    .config("spark.sql.shuffle.partitions", "4") \
    .getOrCreate()

# V√©rifier si la connexion au cluster est bien √©tablie
print("üöÄ Spark connect√© au master:", spark.sparkContext.master)
print("üñ•Ô∏è Nombre de workers disponibles:", spark.sparkContext.defaultParallelism)

try:
    # Lire les donn√©es depuis MongoDB
    df = spark.read.format("mongodb") \
        .option("database", "github_issues") \
        .option("collection", "issues") \
        .load()

    # Afficher les donn√©es
    df.show()

finally:
    # Arr√™ter la session Spark
    spark.stop()
    print("‚úÖ Spark session stopped.")
